#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Divar Watcher â€” Jet Project (3 files only)
Files: bot.py, Requirements.txt, .env

ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
- Ø¯Ø³ØªÙˆØ±Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù…: /add /remove /list /interval /help
- Ù¾Ø§ÛŒØ´ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ URLÙ‡Ø§ÛŒ Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ø¯ÛŒÙˆØ§Ø± (search/list pages)
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
- Ø¨Ø¯ÙˆÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³Ø› state ÙÙ‚Ø· Ø¯Ø± Ø­Ø§ÙØ¸Ù‡â€ŒÛŒ Ø§Ø¬Ø±Ø§ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ø·Ø¨Ù‚ Ø®ÙˆØ§Ø³ØªÙ‡ Ø´Ù…Ø§)
- Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø³Ø§Ø¯Ù‡â€ŒÛŒ Ù†Ø±Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øª + backoff Ø±ÙˆÛŒ Ø®Ø·Ø§Ù‡Ø§
- ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ¹ Ù„ÛŒØ³Øª (Ù…Ø³ÛŒØ± /s/...) Ù¾Ø°ÛŒØ±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯Ø› ØµÙØ­Ø§Øª ØªÚ©ÛŒ (/v/...) Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯

Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§:
- Ù„ÛŒÙ†Ú© ØªÙ…ÛŒØ² Ùˆ Ù‡Ø§ÛŒÙ¾Ø±Ù„ÛŒÙ†Ú©â€ŒØ´Ø¯Ù‡: ğŸ”— [Ù„ÛŒÙ†Ú© Ø¢Ú¯Ù‡ÛŒ](URL)
- Ø§Ø±Ø³Ø§Ù„ Ù‡Ù…Ù‡Ù” Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø¨ÙÚ†â€ŒÙ‡Ø§ÛŒ Û±Û°ØªØ§ÛŒÛŒ Ø¨Ø§ Ù…Ú©Ø« Û³ Ø«Ø§Ù†ÛŒÙ‡â€ŒØ§ÛŒ Ø¨ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ù‚Ø§Ø¨Ù„â€ŒØªÙ†Ø¸ÛŒÙ… Ø§Ø² .env)
- ØºÛŒØ±ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
- Fail-safe Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ÛŒ MarkdownV2
- Ù…Ù†Ø·Ù‚ Ø¬Ø¯ÛŒØ¯ Ù‚ÛŒÙ…Øª/Ú©Ø§Ø±Ú©Ø±Ø¯ ÙˆÛŒÚ˜Ù‡Ù” Ø¯Ø³ØªÙ‡Ù” Ø®ÙˆØ¯Ø±Ùˆ:
  * Ù‚ÛŒÙ…Øª ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ Ù¾Ø°ÛŒØ±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø­Ø§ÙˆÛŒ Â«ØªÙˆÙ…Ø§Ù†Â» ÛŒØ§ Â«Ù‚ÛŒÙ…Øª ØªÙˆØ§ÙÙ‚ÛŒ/ØªÙˆØ§ÙÙ‚ÛŒÂ» Ø¨Ø§Ø´Ø¯.
  * Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ù…Ù„ Â«Ú©ÛŒÙ„ÙˆÙ…ØªØ±Â» ÛŒØ§ Â«kmÂ» Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† mileage Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù‡Ø±Ú¯Ø² Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ù‚ÛŒÙ…Øª Ù‚Ø±Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯.
  * Ø§Ú¯Ø± Ù‚ÛŒÙ…Øª ÛŒØ§ÙØª Ù†Ø´ÙˆØ¯: Â«â€”Â» Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
  * Ø§Ú¯Ø± Ú©Ø§Ø±Ú©Ø±Ø¯ ÛŒØ§ÙØª Ø´ÙˆØ¯: Â«Ú©Ø§Ø±Ú©Ø±Ø¯: ...Â» Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

Ù†Ú©ØªÙ‡Ù” JobQueue:
  pip install 'python-telegram-bot[job-queue]==21.6'
"""

import asyncio
import os
import re
from typing import Dict, Set, List, Tuple, Iterable
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import logging

import httpx
from bs4 import BeautifulSoup

# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ .env Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ø§Ø¬Ø±Ø§ ---
from dotenv import load_dotenv
load_dotenv()  # .env Ø¯Ø± Ù‡Ù…Ø§Ù† Ù¾ÙˆØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

from telegram import Update
from telegram.constants import ParseMode
from telegram.error import BadRequest
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    ContextTypes,
)

from datetime import datetime
from persiantools.jdatetime import JalaliDate
# --------------------------- Config & Globals ---------------------------

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "").strip()
DEFAULT_INTERVAL_MIN = int(os.getenv("DEFAULT_INTERVAL_MIN", "3"))
ALLOWED_CHAT_ID = os.getenv("ALLOWED_CHAT_ID", "").strip()  # Ø§Ø®ØªÛŒØ§Ø±ÛŒ: Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ ÛŒÚ© Ú†Øª/Ú©Ø§Ù†Ø§Ù„
HTTP_TIMEOUT = float(os.getenv("HTTP_TIMEOUT", "15"))
HTTP_RETRIES = int(os.getenv("HTTP_RETRIES", "2"))
USER_AGENT = os.getenv(
    "USER_AGENT",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø±Ø³Ø§Ù„
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "10"))
BATCH_PAUSE_SEC = float(os.getenv("BATCH_PAUSE_SEC", "3"))  # Ù…Ú©Ø« Ø¨ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨ÙÚ†

LOG = logging.getLogger("Divar")
# Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ø¯Ø±ÙˆÙ†â€ŒØ­Ø§ÙØ¸Ù‡â€ŒØ§ÛŒ:
# chats_state = {
#   chat_id: {
#       "feeds": set([url, ...]),
#       "seen": { url: set([ad_id, ...]) },
#       "interval": minutes
#   }
# }
chats_state: Dict[int, Dict] = {}

# Ø¨Ø±Ø§ÛŒ jobâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡â€ŒÛŒ Ù‡Ø± Ú†Øª:
# jobs[chat_id] = job
jobs: Dict[int, object] = {}

DIVAR_HOST = "divar.ir"

# --------------------------- Utilities ---------------------------

def normalize_url(u: str) -> str:
    """ÛŒÙˆÙ†ÛŒÙÙˆØ±Ù… Ú©Ø±Ø¯Ù† URL (Ø­Ø°Ù fragmentØŒ sort Ú©Ø±Ø¯Ù† queryÙ‡Ø§ØŒ Ø­Ø°Ù slash Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø·Ø¨Ù‚ Ù†ÛŒØ§Ø²)"""
    u = u.strip()
    parsed = urlparse(u)
    # Ø§Ú¯Ø± Ù¾Ø±ÙˆØªÚ©Ù„ Ù†Ø¯Ø§Ø´ØªØŒ https Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
    scheme = parsed.scheme or "https"
    netloc = parsed.netloc or DIVAR_HOST
    path = parsed.path or "/"
    # Ø­Ø°Ù fragment
    fragment = ""
    # query Ù…Ø±ØªØ¨â€ŒØ´Ø¯Ù‡
    q_pairs = parse_qsl(parsed.query, keep_blank_values=True)
    q_pairs.sort()
    query = urlencode(q_pairs)

    # Ø­Ø°Ù slash Ø§Ø¶Ø§ÙÙ‡ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ ÙÙ‚Ø· "/"
    if path != "/" and path.endswith("/"):
        path = path[:-1]

    normalized = urlunparse((scheme, netloc, path, "", query, fragment))
    return normalized


def is_divar_search_url(u: str) -> bool:
    """ÙÙ‚Ø· ØµÙØ­Ø§Øª Ù„ÛŒØ³Øª Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ù‚Ø§Ø¨Ù„ Ù¾Ø§ÛŒØ´ Ù‡Ø³ØªÙ†Ø¯: Ø¯Ø§Ù…Ù†Ù‡ divar.ir Ùˆ Ù…Ø³ÛŒØ± /s/"""
    try:
        parsed = urlparse(u)
        return (parsed.netloc or "").endswith(DIVAR_HOST) and parsed.path.startswith("/s/")
    except Exception:
        return False


def is_divar_single_ad(u: str) -> bool:
    """ØµÙØ­Ù‡ ØªÚ©ÛŒ Ø¢Ú¯Ù‡ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ø³ÛŒØ± /v/... Ø¯Ø§Ø±Ø¯."""
    try:
        parsed = urlparse(u)
        return parsed.path.startswith("/v/")
    except Exception:
        return False


def clean_text(txt: str) -> str:
    return re.sub(r"\s+", " ", txt or "").strip()


def parse_price_and_mileage(texts: List[str]) -> Tuple[str, str]:
    """
    Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø§Ø±ØªØŒ Ù‚ÛŒÙ…Øª Ùˆ Ú©Ø§Ø±Ú©Ø±Ø¯ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    - Ù‚ÛŒÙ…Øª ÙÙ‚Ø· Ø§Ú¯Ø± Ø´Ø§Ù…Ù„ 'ØªÙˆÙ…Ø§Ù†' ÛŒØ§ 'Ù‚ÛŒÙ…Øª ØªÙˆØ§ÙÙ‚ÛŒ' ÛŒØ§ 'ØªÙˆØ§ÙÙ‚ÛŒ' Ø¨Ø§Ø´Ø¯ Ù¾Ø°ÛŒØ±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    - mileage Ø§Ú¯Ø± Ø´Ø§Ù…Ù„ 'Ú©ÛŒÙ„ÙˆÙ…ØªØ±' ÛŒØ§ 'km' Ø¨Ø§Ø´Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    Ø®Ø±ÙˆØ¬ÛŒ: (price, mileage)
    """
    price = ""
    mileage = ""

    re_mileage = re.compile(r"(?:(?:Ú©Ø§Ø±Ú©Ø±Ø¯|Ú©ÛŒÙ„ÙˆÙ…ØªØ±|km)\s*[:ï¼š]?\s*)?([\d,\.]+)\s*(?:Ú©ÛŒÙ„ÙˆÙ…ØªØ±|km)", re.IGNORECASE)
    re_price = re.compile(r"(?:^|\s)(?:Ù‚ÛŒÙ…Øª[:ï¼š]?\s*)?([\d,\.]+(?:\s*,\s*[\d\.]+)*)\s*ØªÙˆÙ…Ø§Ù†", re.IGNORECASE)
    re_price_tavafoghi = re.compile(r"(?:Ù‚ÛŒÙ…Øª\s*)?(?:ØªÙˆØ§ÙÙ‚ÛŒ|Ù‚ÛŒÙ…Øª ØªÙˆØ§ÙÙ‚ÛŒ)", re.IGNORECASE)

    for raw in texts:
        t = clean_text(raw)

        if not mileage:
            m = re_mileage.search(t)
            if m:
                val = m.group(1)
                mileage = f"{val} Ú©ÛŒÙ„ÙˆÙ…ØªØ±"

        if not price:
            m2 = re_price.search(t)
            if m2:
                num = m2.group(1)
                price = f"{num} ØªÙˆÙ…Ø§Ù†"

        if not price and re_price_tavafoghi.search(t):
            price = "Ù‚ÛŒÙ…Øª ØªÙˆØ§ÙÙ‚ÛŒ"

    return price, mileage


def extract_ads_from_html(html: str, base_url: str) -> List[Dict]:
    """
    ØªÙ„Ø§Ø´ Ù…Ù‚Ø§ÙˆÙ… Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø§Ø² HTML Ù†ØªØ§ÛŒØ¬.
    - Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ /v/ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
    - title/price/location/mileage Ø¨Ø§ best-effort Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.
    """
    soup = BeautifulSoup(html, "lxml")

    ads = []
    seen_href: Set[str] = set()

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if not href.startswith("/v/"):
            continue
        if href in seen_href:
            continue
        seen_href.add(href)

        ad_id = href.split("/v/", 1)[-1].split("?")[0].strip("/")

        # Ú©Ø§Ø±Øª ÙˆØ§Ù„Ø¯
        card = a
        for _ in range(4):
            if card and (card.name in ("article", "div", "li")):
                break
            card = card.parent

        title = None
        price = ""
        location = None
        mileage = ""

        if a.get_text(strip=True):
            title = a.get_text(strip=True)

        texts_for_parse: List[str] = []
        if card:
            for selector in ["h2", "h3", "h4", ".kt-post-card__title", "[data-test-id='title']"]:
                el = card.select_one(selector)
                if el and el.get_text(strip=True):
                    title = el.get_text(strip=True)
                    break

            for selector in [
                ".kt-post-card__description",
                ".kt-post-card__price",
                ".kt-post-card__bottom-description",
                "[data-test-id='price']",
                "[data-test-id='location']",
                "[class*='post-card']",
            ]:
                for el in card.select(selector):
                    txt = el.get_text(" ", strip=True)
                    if txt:
                        texts_for_parse.append(txt)

        if texts_for_parse:
            p, m = parse_price_and_mileage(texts_for_parse)
            if p:
                price = p
            if m:
                mileage = m

            for t in texts_for_parse:
                tt = clean_text(t)
                if ("Ú©ÛŒÙ„ÙˆÙ…ØªØ±" in tt) or ("km" in tt.lower()) or ("ØªÙˆÙ…Ø§Ù†" in tt) or ("Ù‚ÛŒÙ…Øª" in tt):
                    continue
                if "Ø¯Ø± " in tt or "Ù…Ø­Ù„Ù‡" in tt or "Ù…Ù†Ø·Ù‚Ù‡" in tt:
                    location = tt
                    break

        parsed_base = urlparse(base_url)
        full_url = urlunparse((parsed_base.scheme, parsed_base.netloc, href, "", "", ""))

        ads.append(
            {
                "id": ad_id or href,
                "url": full_url,
                "title": title or "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†",
                "price": price or "â€”",
                "location": location or "â€”",
                "mileage": mileage or "",
            }
        )

    return ads


async def fetch_search_page(client: httpx.AsyncClient, url: str) -> Tuple[str, int]:
    """Ø¯Ø±ÛŒØ§ÙØª HTML ØµÙØ­Ù‡ Ø¨Ø§ backoff Ø³Ø§Ø¯Ù‡Ø› Ø®Ø±ÙˆØ¬ÛŒ: (Ù…ØªÙ†ØŒ status_code)"""
    last_exc = None
    for attempt in range(HTTP_RETRIES + 1):
        try:
            resp = await client.get(
                url,
                headers={"User-Agent": USER_AGENT, "Accept-Language": "fa-IR,fa;q=0.9,en;q=0.8"},
                timeout=HTTP_TIMEOUT,
            )
            return resp.text, resp.status_code
        except Exception as e:
            last_exc = e
            await asyncio.sleep(min(2 ** attempt, 5))
    raise last_exc if last_exc else RuntimeError("Fetch failed")


def ensure_chat(chat_id: int):
    if chat_id not in chats_state:
        chats_state[chat_id] = {"feeds": set(), "seen": {}, "interval": DEFAULT_INTERVAL_MIN}


def get_job_for_chat(context: ContextTypes.DEFAULT_TYPE, chat_id: int):
    return jobs.get(chat_id)


def schedule_or_update_job(application, chat_id: int):
    job = jobs.get(chat_id)
    if job:
        job.schedule_removal()

    interval_min = max(1, int(chats_state[chat_id]["interval"]))

    if application.job_queue is None:
        raise RuntimeError(
            "JobQueue Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install 'python-telegram-bot[job-queue]==21.6'"
        )

    job = application.job_queue.run_repeating(
        callback=poll_chat_feeds,
        interval=interval_min * 60,
        first=5,
        data={"chat_id": chat_id},
        name=f"poll_{chat_id}",
    )
    jobs[chat_id] = job


# --------------------------- Telegram Handlers ---------------------------

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await check_access(update):
        return
    ensure_chat(update.effective_chat.id)
    text = (
        "Ø³Ù„Ø§Ù…! ğŸ‘‹\n"
        "Ù…Ù† Ø±Ø¨Ø§Øª Ù¾Ø§ÛŒØ´ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÙˆØ§Ø± Ù‡Ø³ØªÙ….\n\n"
        "Ø¯Ø³ØªÙˆØ±Ù‡Ø§:\n"
        "/add <url> â€“ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ø¯ÛŒÙˆØ§Ø± (Ù†ÙˆØ¹ /s/)\n"
        "/remove <url> â€“ Ø­Ø°Ù Ù„ÛŒÙ†Ú©\n"
        "/list â€“ Ù†Ù…Ø§ÛŒØ´ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§ÛŒØ´\n"
        "/interval <minutes> â€“ ØªØ¹ÛŒÛŒÙ† ÙØ§ØµÙ„Ù‡â€ŒÛŒ Ù¾Ø§ÛŒØ´ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: {min} Ø¯Ù‚ÛŒÙ‚Ù‡)\n"
        "/help â€“ Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§\n\n"
        "Ù‚Ø§Ù†ÙˆÙ† Ø§ØµÙ„ÛŒ: ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± Â«Ù„ÛŒØ³Øª Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§Â» Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ùˆ Ù…Ø³ÛŒØ±Ø´Ø§Ù† Ø¨Ø§ /s/ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù‚Ø§Ø¨Ù„ Ù¾Ø§ÛŒØ´ Ù‡Ø³ØªÙ†Ø¯."
    ).format(min=DEFAULT_INTERVAL_MIN)
    await update.effective_message.reply_text(text)


async def help_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await check_access(update):
        return
    await start(update, context)


async def add_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await check_access(update):
        return
    ensure_chat(update.effective_chat.id)

    if not context.args:
        await update.effective_message.reply_text("Ù„Ø·ÙØ§Ù‹ Ù¾Ø³ Ø§Ø² /add Ù„ÛŒÙ†Ú© Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ø¯ÛŒÙˆØ§Ø± Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯.")
        return

    raw_url = " ".join(context.args).strip()
    url = normalize_url(raw_url)

    if is_divar_single_ad(url):
        await update.effective_message.reply_text("Ø§ÛŒÙ† Ù„ÛŒÙ†Ú© ÛŒÚ© Ø¢Ú¯Ù‡ÛŒ ØªÚ©ÛŒ (/v/...) Ø§Ø³Øª Ùˆ Ù‚Ø§Ø¨Ù„ Ù¾Ø§ÛŒØ´ Ù†ÛŒØ³Øª.")
        return

    if not is_divar_search_url(url):
        await update.effective_message.reply_text("Ø§ÛŒÙ† Ù„ÛŒÙ†Ú© Ø¨Ù‡ Ù†Ø¸Ø± Ù†Ù…ÛŒâ€ŒØ±Ø³Ø¯ ØµÙØ­Ù‡Ù” Ù†ØªØ§ÛŒØ¬ (/s/...) Ø¨Ø§Ø´Ø¯. ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª Ù‚Ø§Ø¨Ù„ Ù¾Ø§ÛŒØ´ Ù‡Ø³ØªÙ†Ø¯.")
        return

    chats_state[update.effective_chat.id]["feeds"].add(url)
    chats_state[update.effective_chat.id]["seen"].setdefault(url, set())
    schedule_or_update_job(context.application, update.effective_chat.id)

    await update.effective_message.reply_text(f"âœ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯:\n{url}")


async def remove_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await check_access(update):
        return
    ensure_chat(update.effective_chat.id)

    if not context.args:
        await update.effective_message.reply_text("Ù„Ø·ÙØ§Ù‹ Ù¾Ø³ Ø§Ø² /removeØŒ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…Ø§Ù† Ù„ÛŒÙ†Ú©ÛŒ Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯.")
        return

    # âœ… ØªØ§ÛŒÙ¾ÙˆÛŒ Ù‚Ø¨Ù„ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯: " ".join(...)
    raw_url = " ".join(context.args).strip()
    url = normalize_url(raw_url)

    feeds: Set[str] = chats_state[update.effective_chat.id]["feeds"]
    if url in feeds:
        feeds.remove(url)
        chats_state[update.effective_chat.id]["seen"].pop(url, None)
        await update.effective_message.reply_text(f"ğŸ—‘ Ø­Ø°Ù Ø´Ø¯:\n{url}")
    else:
        await update.effective_message.reply_text("Ú†Ù†ÛŒÙ† Ù„ÛŒÙ†Ú©ÛŒ Ø¯Ø± Ù„ÛŒØ³Øª Ù¾Ø§ÛŒØ´ Ù†Ø¨ÙˆØ¯.")


async def list_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await check_access(update):
        return
    ensure_chat(update.effective_chat.id)

    feeds: Set[str] = chats_state[update.effective_chat.id]["feeds"]
    if not feeds:
        await update.effective_message.reply_text("Ù‡Ù†ÙˆØ² Ù„ÛŒÙ†Ú©ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯. Ø¨Ø§ Ø¯Ø³ØªÙˆØ± /add Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒØ¯.")
        return

    lines = ["ğŸ” Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§ÛŒØ´:"]
    for u in sorted(feeds):
        lines.append(f"â€¢ {u}")
    await update.effective_message.reply_text("\n".join(lines))


async def interval_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await check_access(update):
        return
    ensure_chat(update.effective_chat.id)

    if not context.args:
        cur = chats_state[update.effective_chat.id]["interval"]
        await update.effective_message.reply_text(f"ÙØ§ØµÙ„Ù‡â€ŒÛŒ ÙØ¹Ù„ÛŒ Ù¾Ø§ÛŒØ´: {cur} Ø¯Ù‚ÛŒÙ‚Ù‡\nØ¨Ø±Ø§ÛŒ ØªØºÛŒÛŒØ±: /interval 5")
        return

    try:
        m = int(context.args[0])
        if m < 1 or m > 60:
            raise ValueError
    except Exception:
        await update.effective_message.reply_text("Ø¹Ø¯Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª. Ø¨Ø§Ø²Ù‡Ù” Ù…Ø¬Ø§Ø²: 1 ØªØ§ 60.")
        return

    chats_state[update.effective_chat.id]["interval"] = m
    if chats_state[update.effective_chat.id]["feeds"]:
        schedule_or_update_job(context.application, update.effective_chat.id)

    await update.effective_message.reply_text(f"â± ÙØ§ØµÙ„Ù‡â€ŒÛŒ Ù¾Ø§ÛŒØ´ Ø±ÙˆÛŒ {m} Ø¯Ù‚ÛŒÙ‚Ù‡ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯.")


# --------------------------- Send helpers ---------------------------

def escape_md(text: str) -> str:
    """Escape Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ MarkdownV2 (Ø·Ø¨Ù‚ Ù…Ø³ØªÙ†Ø¯Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù…)"""
    if text is None:
        return ""
    return re.sub(r"([_*\[\]()~`>#+\-=|{}.!])", r"\\\1", text)


async def safe_send_markdown(context: ContextTypes.DEFAULT_TYPE, chat_id: int, text: str):
    """
    ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ø§ MarkdownV2Ø› Ø§Ú¯Ø± BadRequest Ø¨Ù‡ Ø®Ø§Ø·Ø± escape Ø±Ø® Ø¯Ø§Ø¯ØŒ
    Ø¨Ø¯ÙˆÙ† parse_mode Ø¨Ù‡ ØµÙˆØ±Øª Ù…ØªÙ† Ø³Ø§Ø¯Ù‡ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    try:
        await context.bot.send_message(
            chat_id=chat_id,
            text=text,
            parse_mode=ParseMode.MARKDOWN_V2,
            disable_web_page_preview=True,
        )
    except BadRequest:
        try:
            await context.bot.send_message(
                chat_id=chat_id,
                text=text,
                disable_web_page_preview=True,
            )
        except Exception:
            pass


def chunk(iterable: List[Dict], size: int) -> Iterable[List[Dict]]:
    """ØªÙ‚Ø³ÛŒÙ… Ù„ÛŒØ³Øª Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ size-ØªØ§ÛŒÛŒ"""
    for i in range(0, len(iterable), size):
        yield iterable[i:i + size]


def build_ad_block(ad: Dict) -> str:
    """
    Ø¨Ù„ÙˆÚ© Ù†Ù…Ø§ÛŒØ´ Ù‡Ø± Ø¢Ú¯Ù‡ÛŒ Ø¨Ø±Ø§ÛŒ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ† Ø¯Ø± ÛŒÚ© Ù¾ÛŒØ§Ù… Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ.
    URL Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ø§ÛŒÙ¾Ø±Ù„ÛŒÙ†Ú© Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (MarkdownV2).
    """
    mileage_line = ""
    if ad.get("mileage"):
        mileage_line = f"\n  Ú©Ø§Ø±Ú©Ø±Ø¯: {escape_md(ad['mileage'])}"

    return (
        f"â€¢ *{escape_md(ad['title'])}*\n"
        f"  Ù‚ÛŒÙ…Øª: {escape_md(ad['price'])}\n"
        f"  Ù…Ú©Ø§Ù†: {escape_md(ad['location'])}"
        f"{mileage_line}\n"
        f"  ğŸ”— [Ù„ÛŒÙ†Ú© Ø¢Ú¯Ù‡ÛŒ]({ad['url']})"
    )


# --------------------------- Polling Job ---------------------------

async def poll_chat_feeds(context: ContextTypes.DEFAULT_TYPE):
    """Job: Ù‡Ø± N Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú†Øª Ù‡Ù…Ù‡â€ŒÛŒ ÙÛŒØ¯Ù‡Ø§ Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    job_data = context.job.data or {}
    chat_id = job_data.get("chat_id")
    if chat_id is None or chat_id not in chats_state:
        return

    feeds: Set[str] = chats_state[chat_id]["feeds"]
    if not feeds:
        return

    async with httpx.AsyncClient(follow_redirects=True) as client:
        for url in list(feeds):
            try:
                html, status = await fetch_search_page(client, url)
                if status != 200:
                    await context.bot.send_message(
                        chat_id=chat_id,
                        text=f"âš ï¸ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø§Ù…ÙˆÙÙ‚ Ø§Ø² Ø¯ÛŒÙˆØ§Ø± ({status}) Ø¨Ø±Ø§ÛŒ:\n{url}",
                        disable_web_page_preview=True,
                    )
                    continue

                ads = extract_ads_from_html(html, url)
                LOG.info("DIVAR: Extracted %d ads for URL: %s", len(ads), url)
                if not ads:
                    continue

                seen_set: Set[str] = chats_state[chat_id]["seen"].setdefault(url, set())

                new_ads = [ad for ad in ads if ad["id"] not in seen_set]

                for ad in ads[:50]:
                    seen_set.add(ad["id"])

                if not new_ads:
                    continue

                for idx, batch in enumerate(chunk(new_ads, BATCH_SIZE), start=1):
                    header = "ğŸ“¢ *Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø¯ÛŒÙˆØ§Ø±*\n"
                    body = "\n\n".join(build_ad_block(ad) for ad in batch)
                    msg_text = header + body
                    await safe_send_markdown(context, chat_id, msg_text)

                    if idx * BATCH_SIZE < len(new_ads):
                        await asyncio.sleep(BATCH_PAUSE_SEC)

            except Exception as e:
                await context.bot.send_message(
                    chat_id=chat_id,
                    text=f"âš ï¸ Ø®Ø·Ø§ Ù‡Ù†Ú¯Ø§Ù… Ù¾Ø§ÛŒØ´:\n{url}\n{type(e).__name__}: {e}",
                    disable_web_page_preview=True,
                )


# --------------------------- Main ---------------------------

async def check_access(update: Update) -> bool:
    """Ø§Ú¯Ø± ALLOWED_CHAT_ID ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ ÙÙ‚Ø· Ù‡Ù…Ø§Ù† Ú†Øª Ù…Ø¬Ø§Ø² Ø§Ø³Øª."""
    if not ALLOWED_CHAT_ID:
        return True
    try:
        allowed = int(ALLOWED_CHAT_ID)
        return update.effective_chat and update.effective_chat.id == allowed
    except Exception:
        return True


def ensure_env():
    if not TELEGRAM_TOKEN:
        raise RuntimeError("Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ TELEGRAM_TOKEN Ø¯Ø± .env ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

async def _get_html(url: str) -> str:
    """Async fetches HTML using Divar's config (timeout, UA)."""
    try:
        async with httpx.AsyncClient(
            timeout=HTTP_TIMEOUT, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØºÛŒØ± Ø³Ø±Ø§Ø³Ø±ÛŒ
            headers={"User-Agent": USER_AGENT}, # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØºÛŒØ± Ø³Ø±Ø§Ø³Ø±ÛŒ
            follow_redirects=True,
            limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
            # retries=HTTP_RETRIES,
        ) as c:
            r = await c.get(url)
            r.raise_for_status() # Ø§ÛŒÙ† Ø®Ø· Ø§Ú¯Ø± Ú©Ø¯ Ù¾Ø§Ø³Ø® 4xx/5xx Ø¨Ø§Ø´Ø¯ØŒ HTTPStatusError ØµØ§Ø¯Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            
        if "html" in (r.headers.get("content-type") or "").lower():
            return r.text or ""
        else:
            LOG.warning("DIVAR: Fetched non-HTML content for %s (Status: %d, Type: %s)", 
                        url, r.status_code, r.headers.get("content-type"))
            return ""
            
    except httpx.HTTPStatusError as e:
        # Ø®Ø·Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØª HTTP (Ù…Ø«Ù„Ø§Ù‹ 403 Forbidden ÛŒØ§ 404 Not Found)
        LOG.error("DIVAR: HTTP Error %s for URL: %s", e.response.status_code, url)
        # ğŸ’¡ Ø§ÛŒÙ† Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ Ù…Ø¹Ù†ÛŒ Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯Ù† Ø±Ø¨Ø§Øª ØªÙˆØ³Ø· Ø¯ÛŒÙˆØ§Ø± Ø§Ø³Øª.
    except Exception as e:
        # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± (Ù…Ø§Ù†Ù†Ø¯ TimeoutØŒ DNS ÛŒØ§ Connection Error)
        # ğŸ’¡ Ø§ÛŒÙ† Ø§ØºÙ„Ø¨ Ø¨Ù‡ Ù…Ø¹Ù†ÛŒ Ù…Ø´Ú©Ù„ Ø¯Ø± Ø´Ø¨Ú©Ù‡ ÛŒØ§ Timeout Ø§Ø³Øª.
        LOG.error("DIVAR: Fetch Failed for URL: %s. Error Type: %s", url, type(e).__name__, exc_info=True)
        
    return ""

import re
from persiantools.jdatetime import JalaliDate

def _escape_md(text: str) -> str:
    """Escape Ø§Ù…Ù† Ø¨Ø±Ø§ÛŒ MarkdownV2 (Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ ØªÙ„Ú¯Ø±Ø§Ù…)"""
    if not text:
        return ""
    text = str(text)
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§
    text = text.strip()
    # escape Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ MarkdownV2
    return re.sub(r'([_*\[\]()~`>#+\-=|{}.!])', r'\\\1', text)



def _num_emoji(i: int) -> str:
    """Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¹Ø¯Ø¯ÛŒ 1ï¸âƒ£ ØªØ§ 9ï¸âƒ£ Ùˆ ğŸ”Ÿ Ø¨Ø±Ø§ÛŒ 10"""
    if 1 <= i <= 9:
        return f"{i}\u20E3"
    elif i == 10:
        return "ğŸ”Ÿ"
    else:
        return str(i)


def _get_jalali_date_str() -> str:
    """ØªØ§Ø±ÛŒØ® Ø§Ù…Ø±ÙˆØ² Ø´Ù…Ø³ÛŒ Ø¨Ø§ Ù†Ø§Ù… Ù…Ø§Ù‡ ÙØ§Ø±Ø³ÛŒ"""
    today = JalaliDate.today()
    months_fa = [
        "ÙØ±ÙˆØ±Ø¯ÛŒÙ†", "Ø§Ø±Ø¯ÛŒØ¨Ù‡Ø´Øª", "Ø®Ø±Ø¯Ø§Ø¯", "ØªÛŒØ±", "Ù…Ø±Ø¯Ø§Ø¯", "Ø´Ù‡Ø±ÛŒÙˆØ±",
        "Ù…Ù‡Ø±", "Ø¢Ø¨Ø§Ù†", "Ø¢Ø°Ø±", "Ø¯ÛŒ", "Ø¨Ù‡Ù…Ù†", "Ø§Ø³ÙÙ†Ø¯"
    ]
    month_name = months_fa[today.month - 1]
    return f"{today.day} {month_name} {today.year}"


def _parse_divar_url(url: str):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø´Ù‡Ø± Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø² URL Ø¯ÛŒÙˆØ§Ø±
    Ù…Ø«Ù„: https://divar.ir/s/tehran/auto/qeytarieh
    """
    pattern = r"https?://divar\.ir/s/([^/]+)/([^/?#]+)"
    m = re.search(pattern, url)
    if not m:
        return ("", "")
    city = m.group(1)
    category = m.group(2)
    return (city, category)

def _map_divar_category(category: str) -> str:
    """ØªØ±Ø¬Ù…Ù‡â€ŒÛŒ slug Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÛŒÙˆØ§Ø± Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø®ÙˆØ§Ù†Ø§"""
    mapping = {
        "auto": "Ø®ÙˆØ¯Ø±Ùˆ",
        "car": "Ø®ÙˆØ¯Ø±Ùˆ",
        "cars": "Ø®ÙˆØ¯Ø±Ùˆ",
        "real-estate": "Ø§Ù…Ù„Ø§Ú©",
        "mobile-phones": "Ù…ÙˆØ¨Ø§ÛŒÙ„ Ùˆ ØªÙ„ÙÙ† Ù‡Ù…Ø±Ø§Ù‡",
        "electronic-devices": "Ú©Ø§Ù„Ø§ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„",
        "home-kitchen": "Ù„ÙˆØ§Ø²Ù… Ø®Ø§Ù†Ú¯ÛŒ",
        "jobs": "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ùˆ Ú©Ø§Ø±ÛŒØ§Ø¨ÛŒ",
        "personal-goods": "ÙˆØ³Ø§ÛŒÙ„ Ø´Ø®ØµÛŒ",
        "entertainment": "ÙØ±Ù‡Ù†Ú¯ÛŒ Ùˆ Ø³Ø±Ú¯Ø±Ù…ÛŒ",
        "services": "Ø®Ø¯Ù…Ø§Øª",
        "animals": "Ø­ÛŒÙˆØ§Ù†Ø§Øª",
        "tools-materials-equipment": "ØªØ¬Ù‡ÛŒØ²Ø§Øª Ùˆ ØµÙ†Ø¹ØªÛŒ",
        "social-services": "Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ",
    }
    return mapping.get(category.lower(), category.replace("-", " ").capitalize())


async def process_divar(store, cid_int, url: str, chat_lang) -> str:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ø¯ÛŒÙˆØ§Ø±:
    - ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
    - Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û° Ø¹Ø¯Ø¯
    - Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù‡Ø± Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
    """
    if not is_divar_search_url(url):
        LOG.warning("DIVAR: URL validation failed for: %s", url)
        return ""

    html = await _get_html(url)
    if not html:
        return ""

    ads = extract_ads_from_html(html, url)
    if not ads:
        return ""

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù‡Ø± Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø² URL
    city, category = _parse_divar_url(url)
    city = city.capitalize() if city else "Ù†Ø§Ù…Ø´Ø®Øµ"
    category = category.capitalize() if category else "â€”"

    seen_key = f"divar_seen::{url}"
    seen_ids = set(store.get_seen(cid_int, seen_key) or [])

    new_ads = [ad for ad in ads if ad["id"] not in seen_ids]
    if not new_ads:
        return ""

    latest_new = new_ads[:10]
    today_jalali = _get_jalali_date_str()

    # Ø³Ø±Ø¢ØºØ§Ø² Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯:
    fa_category = _map_divar_category(category)

    # ØªØ¹Ø¯Ø§Ø¯ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
    count = len(latest_new)
    count_fa = str(count)  # Ø§Ú¯Ø± Ø®ÙˆØ§Ø³ØªÛŒ Ø¨Ø¹Ø¯Ø§Ù‹ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø¨Ø´Ù†ØŒ Ù…ÛŒØ´Ù‡ Ø¬Ø¯Ø§ Ù†ÙˆØ´Øª

    # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ Ù‡Ø´ØªÚ¯ ØªÙ…ÛŒØ² (ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ _ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´Ù†)
    fa_category_tag = "#" + fa_category.replace(" ", "_")

    # Ù¾ÛŒØ§Ù… Ø¨Ø§Ù„Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
    header = f"{count_fa} Ø¢Ú¯Ù‡ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ {fa_category_tag}\n\n"

    lines = []
    for i, ad in enumerate(latest_new, start=1):
        num_emoji = _num_emoji(i)
        title = _escape_md(ad.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"))
        price = _escape_md(ad.get("price", "â€”"))
        loc = _escape_md(ad.get("location", "â€”"))
        mileage = _escape_md(ad.get("mileage", ""))
        url_md = ad["url"].replace(")", "\\)").replace("(", "\\(")


        part = (
            f"{num_emoji} *{title}*\n"
            f"_{_escape_md('Ø¯ÛŒÙˆØ§Ø±')} \\| {today_jalali}_\n\n"
            f" Ù‚ÛŒÙ…Øª: {price}\n"
            f" Ù…Ú©Ø§Ù†: {loc}"
        )
        if mileage:
            part += f"\n  Ú©Ø§Ø±Ú©Ø±Ø¯: {mileage}"
        part += f"\nğŸ”— [Ù„ÛŒÙ†Ú© Ø¢Ú¯Ù‡ÛŒ]({url_md})"
        lines.append(part)
        seen_ids.add(ad["id"])

    store.set_seen(cid_int, seen_key, seen_ids)

    msg = header + "\n\n".join(lines)
    return msg

# def main():
#     ensure_env()

#     application = ApplicationBuilder().token(TELEGRAM_TOKEN).build()

#     application.add_handler(CommandHandler("start", start))
#     application.add_handler(CommandHandler("help", help_cmd))
#     application.add_handler(CommandHandler("add", add_cmd))
#     application.add_handler(CommandHandler("remove", remove_cmd))
#     application.add_handler(CommandHandler("list", list_cmd))
#     application.add_handler(CommandHandler("interval", interval_cmd))

#     print("Bot is running (long polling)...")
#     application.run_polling()


# if __name__ == "__main__":
#     main()
